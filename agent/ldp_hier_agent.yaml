_target_: agent.ldp_hier_agent.LDPHierAgent
name: ldp_hier_agent

vae_pretrain_path: null
vae_feature_dim: 16

planner:
  _target_: networks.diffusion_nets_v2.ConditionalUnet1D
  input_dim: ??? # to be specified later
  global_cond_dim: ??? # to be specified later
  diffusion_step_embed_dim: 256
  down_dims: [256, 512, 1024]
  kernel_size: 5
  n_groups: 8
  downsample: True


idm_net:
  _target_: networks.diffusion_nets_v2.ConditionalUnet1D
  input_dim: ??? # to be specified later
  global_cond_dim: ??? # to be specified later
  diffusion_step_embed_dim: 256
  down_dims: [256, 512]
  kernel_size: 5
  n_groups: 8
  downsample: True
  
# training
use_planner: True
use_idm: True 

# policy input
lowdim_obs: ${data.meta.lowdim_obs}
rgb_obs: ${data.meta.rgb_obs}
data_name: ${data.name}
obs_normalization: ${data.meta.obs_normalization}

# policy details
obs_horizon: ${obs_horizon}
pred_horizon: ${eval:'${horizon}-1'}
action_horizon: ${action_horizon}
idm_horizon: ${idm_horizon}
planner_n_diffusion_steps: 100 # maybe change?
idm_n_diffusion_steps: 100

# training
alpha_planner: 1
alpha_idm: 1
update_planner_every: 1
update_planner_until: -1
update_planner_after: -1
update_idm_every: 1
update_idm_after: -1
lr: ${lr}
end_lr: ${end_lr}
idm_lr: ${lr}
idm_end_lr: ${end_lr}

warmup_steps: ${warmup_steps}
decay_steps: ${n_grad_steps}
grad_clip: 100 # rlly high so it never activates